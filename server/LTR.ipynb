{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import *\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import *\n",
    "from whoosh import scoring\n",
    "from bs4 import BeautifulSoup as bs4 \n",
    "import os\n",
    "import json\n",
    "INDEX_PATH = \"./indexdir\"\n",
    "INDEX_NAME = \"papers\"\n",
    "\n",
    "idx = open_dir(INDEX_PATH, indexname=INDEX_NAME)\n",
    "\n",
    "from whoosh import qparser, query, scoring\n",
    "from whoosh.analysis import RegexTokenizer\n",
    "from whoosh.lang.morph_en import variations\n",
    "\n",
    "freq_searcher = idx.searcher(weighting=scoring.Frequency())\n",
    "tfidf_searcher = idx.searcher(weighting=scoring.TF_IDF())\n",
    "bm25_searcher = idx.searcher(weighting=scoring.BM25F(B=0.74, K1=1.52))\n",
    "query_parser = QueryParser('abstract', idx.schema)\n",
    "query_parser.add_plugin(FuzzyTermPlugin())\n",
    "title_parser = QueryParser('title', idx.schema)\n",
    "title_parser.add_plugin(FuzzyTermPlugin())\n",
    "tokenizer = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genearte_LETOR_data(qid, cur_query, docid, content, title, rel) :\n",
    "    terms = cur_query.split(' ')\n",
    "    q = query_parser.parse(\"path:\\'\"+docid+\"\\' \"+\" OR \".join(terms))\n",
    "    q_2 = title_parser.parse(\"path:\\'\"+docid+\"\\' \"+\" OR \".join(terms))\n",
    "    \n",
    "    results = freq_searcher.search(q, limit=None)\n",
    "    tf_a = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Abstract TF feature\", results[0].score)\n",
    "        tf_a = results[0].score\n",
    "    \n",
    "    results = freq_searcher.search(q_2, limit=None)\n",
    "    tf_t = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Title TF feature\", results[0].score)\n",
    "        tf_t = results[0].score\n",
    "\n",
    "    idf_a = sum(freq_searcher.idf(\"abstract\", x) for x in terms)\n",
    "#     print(\"Abstract IDF feature\", idf_a)\n",
    "    idf_t = sum(freq_searcher.idf(\"title\", x) for x in terms)\n",
    "#     print(\"Title IDF feature\", idf_t)\n",
    "\n",
    "    results = tfidf_searcher.search(q, limit=None)\n",
    "    tfidf_a = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Abstract TF-IDF feature\", results[0].score)\n",
    "        tfidf_a = results[0].score\n",
    "    \n",
    "    results = tfidf_searcher.search(q_2, limit=None)\n",
    "    tfidf_t = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Title TF-IDF feature\", results[0].score)\n",
    "        tfidf_t = results[0].score\n",
    "\n",
    "    results = bm25_searcher.search(q, limit=None)\n",
    "    bm25_a = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Abstract BM25 feature\", results[0].score)\n",
    "        bm25_a = results[0].score\n",
    "    \n",
    "    results = bm25_searcher.search(q_2, limit=None)\n",
    "    bm25_t = 0.0\n",
    "    if len(results):\n",
    "#         print(\"Title BM25 feature\", results[0].score)\n",
    "        bm25_t = results[0].score\n",
    "\n",
    "    dl = len(list(x for x in tokenizer(content)))\n",
    "#     print(\"DL feature\", dl)\n",
    "    \n",
    "    tl = len(list(x for x in tokenizer(title)))\n",
    "#     print(\"TL feature\", tl)\n",
    "    \n",
    "    return rel + \" qid:%s 1:%f 2:%f 3:%f 4:%f 5:%f 6:%f 7:%f 8:%f 9:%f 10:%f #docid = %s\\n\" % (qid, tf_a, tf_t, idf_a, idf_t, tfidf_a, tfidf_t, bm25_a, bm25_t, dl, tl, docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the query map\n",
    "query_map = dict()\n",
    "with open(\"queries.indri\", 'r') as q:\n",
    "    soup = bs4(q, 'lxml')\n",
    "    x = soup.find_all('query')\n",
    "    i = 1\n",
    "    for query in x :\n",
    "        query_1 = query.text.strip().split('\\n')\n",
    "        query_map[str(i)] = query_1[1]\n",
    "        i += 1\n",
    "\n",
    "with open(\"./bob_acl_anth.qrels\", 'r') as qrels, open('train_f.txt', 'w+') as w :\n",
    "    for line in qrels :\n",
    "        data = line.strip().split('\\t')\n",
    "        qid = data[0]\n",
    "        docid = data[2]\n",
    "        rel = data[3]\n",
    "        cur_query = query_map[qid]\n",
    "        stored = freq_searcher.document(path=docid+\".tei.xml\")\n",
    "        if not stored :\n",
    "            continue\n",
    "        w.write(genearte_LETOR_data(qid, cur_query, docid+\".tei.xml\", stored['abstract'], stored['title'], rel))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score  OOB Improve    Remaining                           Monitor Output \n",
      "    1       0.3836       0.2530       42.20s                                         \n",
      "    2       0.4675       0.0969       40.38s                                         \n",
      "    3       0.5149       0.0529       50.91s                                         \n",
      "    4       0.5620      -0.0088       55.68s                                         \n",
      "    5       0.5000      -0.0085       53.19s                                         \n",
      "    6       0.5395       0.0027       50.69s                                         \n",
      "    7       0.5626      -0.0146       48.49s                                         \n",
      "    8       0.5195       0.0005       46.94s                                         \n",
      "    9       0.5353      -0.0043       46.02s                                         \n",
      "   10       0.4942       0.0030       45.35s                                         \n",
      "   15       0.5700       0.0091       42.37s                                         \n",
      "   20       0.5900      -0.0007       44.47s                                         \n",
      "   25       0.5691      -0.0056       44.38s                                         \n",
      "   30       0.5873      -0.0051       42.61s                                         \n",
      "   35       0.5674      -0.0004       41.79s                                         \n",
      "   40       0.5535       0.0023       41.20s                                         \n",
      "   45       0.6452      -0.0000       40.31s                                         \n",
      "   50       0.6270       0.0031       39.54s                                         \n",
      "   60       0.6229       0.0002       38.35s                                         \n",
      "   70       0.6114      -0.0016       38.41s                                         \n",
      "   80       0.5887      -0.0009       38.99s                                         \n",
      "   90       0.5991      -0.0112       39.61s                                         \n",
      "  100       0.6512      -0.0012       40.71s                                         \n",
      "  120       0.6752      -0.0004       39.86s                                         \n",
      "  140       0.6169       0.0014       38.34s                                         \n",
      "  160       0.6361      -0.0006       37.28s                                         \n",
      "  180       0.5921      -0.0012       36.04s                                         \n",
      "  200       0.6027      -0.0025       34.63s                                         \n",
      "  220       0.6550      -0.0001       33.22s                                         \n",
      "  240       0.6367      -0.0024       31.86s                                         \n",
      "  260       0.6759       0.0018       30.34s                                         \n",
      "  280       0.6862      -0.0028       28.86s                                         \n",
      "  300       0.7073       0.0023       27.45s                                         \n",
      "  320       0.6728      -0.0000       26.25s                                         \n",
      "  340       0.6883      -0.0001       25.61s                                         \n",
      "  360       0.6875       0.0028       24.50s                                         \n",
      "  380       0.7280      -0.0009       23.55s                                         \n",
      "  400       0.6812      -0.0053       22.45s                                         \n",
      "  420       0.6876      -0.0017       21.25s                                         \n",
      "  440       0.6719       0.0002       20.09s                                         \n",
      "  460       0.7065       0.0017       18.89s                                         \n",
      "  480       0.6952      -0.0002       17.67s                                         \n",
      "  500       0.7437      -0.0018       16.47s                                         \n",
      "  550       0.7228       0.0003       13.79s                                         \n",
      "  600       0.7136      -0.0022       11.09s                                         \n",
      "  650       0.7417      -0.0000        8.31s                                         \n",
      "  700       0.6898       0.0001        5.51s                                         \n",
      "  750       0.7312       0.0002        2.75s                                         \n",
      "  800       0.7360      -0.0025        0.00s                                         \n"
     ]
    }
   ],
   "source": [
    "# Train data using Learning-To-Rank LambdaMART model\n",
    "import pyltr\n",
    "\n",
    "with open('train_f.txt') as f :\n",
    "    features, rels, qids, comment = pyltr.data.letor.read_dataset(f)\n",
    "    metric = pyltr.metrics.NDCG(k=10)\n",
    "    model = pyltr.models.LambdaMART(\n",
    "        metric=metric,\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.02,\n",
    "        max_features=0.5,\n",
    "        query_subsample=0.5,\n",
    "        max_leaf_nodes=10,\n",
    "        min_samples_leaf=64,\n",
    "        verbose=1,\n",
    "    )\n",
    "    model.fit(features, rels, qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model\n",
    "import pickle\n",
    "\n",
    "with open('ltr_model.pickle', 'wb') as f:\n",
    "    # Pickle the 'ltr_model' dictionary using the highest protocol available.\n",
    "    pickle.dump(model, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ltr_model.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh import qparser, query, scoring\n",
    "\n",
    "words = \"information\"\n",
    "with open('result_f.txt.tmp', 'w+') as wr :\n",
    "    searcher = idx.searcher(weighting=scoring.BM25F(B=0.74, K1=1.52))\n",
    "    query_parser = MultifieldParser(['title', 'abstract', 'area'], idx.schema, termclass=query.Variations)\n",
    "    query_parser.add_plugin(FuzzyTermPlugin())\n",
    "    query_parsed = query.And([query.Variations('abstract', x) for x in words.split(' ') if len(x) > 1])\n",
    "    results = searcher.search(query_parsed, limit=100)\n",
    "    for r in results :\n",
    "        wr.write(genearte_LETOR_data('1', words, r['path'], r['abstract'], r['title'], '1'))\n",
    "\n",
    "with open('result_f.txt.tmp', 'r') as f :\n",
    "    features, _, qids, docs = pyltr.data.letor.read_dataset(f)\n",
    "    p = model.predict(features)\n",
    "    result = []\n",
    "    for j in range(len(qids)) :\n",
    "        result.append((p[j], searcher.document(path=docs[j][8:].strip())))\n",
    "    result.sort(key = lambda x:x[0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docnum = searcher.document_number(path=u\"W05-0307.tei.xml\")\n",
    "r = searcher.more_like(docnum, 'abstract', top = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
